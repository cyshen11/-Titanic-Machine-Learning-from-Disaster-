{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Credit to VK_ds"},{"metadata":{},"cell_type":"markdown","source":"**Load the Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom IPython.display import display\npd.options.display.max_columns = None\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('../input/train.csv')\ntest_df=pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Dictionary\nVariable Definition Key\n\nsurvival Survival 0 = No, 1 = Yes\n\npclass Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex Male or Female\n\nAge Age in years\n\nsibsp # of siblings / spouses aboard the Titanic\n\nparch # of parents / children aboard the Titanic\n\nticket Ticket number\n\nfare Passenger fare\n\ncabin Cabin number\n\nembarked Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\nVariable Notes pclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fiancÃ©s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missingdata(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n    ms = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    ms = ms[ms['Percent'] > 0]\n    f, ax = plt.subplots(figsize=(8,6))\n    plt.xticks(rotation='90')\n    fig = sns.barplot(ms.index, ms['Percent'], color='green', alpha=0.8)\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('Percent missing data by feature', fontsize=15)\n    return ms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missingdata(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missingdata(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filling missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Age'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Fare'].fillna(test_df['Fare'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cabin feature has more than 75% of missing data in both Test and Train data.\nSo we are removing Cabin feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_column = ['Cabin']\ntrain_df.drop(drop_column, axis=1, inplace=True)\ntest_df.drop(drop_column, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both the test and train Age feature contains more than 15% of missing data.\nSo we fill in the missing data with median."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Age'].fillna(test_df['Age'].median(), inplace=True)\ntrain_df['Age'].fillna(test_df['Age'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('check the nan value in train data')\nprint(train_df.isnull().sum())\nprint('___'*30)\nprint('check the nan value in test data')\nprint(test_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Engineering**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine test and train as single to apply function\nall_data = [train_df, test_df]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in all_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search('([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in all_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n\n# Group all non-common titles into one single grouping 'Rare'\nfor dataset in all_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', \n                                                 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create bin for age features\nfor dataset in all_data:\n    dataset['Age_bin'] = pd.cut(dataset['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create bin for fare features\nfor dataset in all_data:\n    dataset['Fare_bin'] = pd.cut(dataset['Fare'], bins=[0,7.91,14.45,31,120], labels=['Low_fare','Median_fare','Average_fare','High_fare'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for our reference, making a copy of both DataSet.\n# Start working on dataset copy.\ntraindf = train_df\ntestdf = test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = [traindf, testdf]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in all_data:\n    drop_column  = ['Age', 'Fare', 'Name', 'Ticket']\n    dataset.drop(drop_column, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_column = ['PassengerId']\ntraindf.drop(drop_column, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now everything almost ready only one step we convert the categorical features in numberical by using dummy variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"testdf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf = pd.get_dummies(traindf, columns = ['Sex','Title','Age_bin','Embarked','Fare_bin'],\n                         prefix=['Sex','Title','Age_type','Em_type','Fare_type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdf = pd.get_dummies(testdf, columns = ['Sex','Title','Age_bin','Embarked','Fare_bin'],\n                        prefix=['Sex','Title','Age_type','Em_type','Fare_type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation between the Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(traindf.corr(), annot=True, cmap='RdYlGn',linewidths=0.2) #data.corr() --> correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pairplots**\n\nFinally let us generate some pairplots to observe the distribution of data from one feature to the other. Once again we use Seaborn to help us."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.pairplot(data=train_df, hue='Survived', palette = 'seismic',\n                 size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nall_features = traindf.drop(\"Survived\", axis=1)\nTargeted_feature = traindf['Survived']\nX_train, X_test, y_train, y_test = train_test_split(all_features,Targeted_feature, test_size=0.3, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\nprediction_lr =  model.predict(X_test)\nprint('-----------------Accuracy of the model-----------------')\nprint('The accuracy of Logistic Regression is', round(accuracy_score(prediction_lr,y_test)*100,2))\nkfold = KFold(n_splits=10,random_state=22)  # k=10, split the data into 10 equal parts\nresult_lr = cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Logistic Regression is:',round(result_lr.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap='summer')\nplt.title('Confusion_matrix',y=1.05,size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from  sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(criterion='gini', n_estimators=700, min_samples_split=10,min_samples_leaf=1,max_features='auto', oob_score=True,\n                              random_state=1,n_jobs=-1)\nmodel.fit(X_train,y_train)\nprediction_rm = model.predict(X_test)\nprint('-----------------Accuracy of the model-----------------')\nprint('The accuracy of Random Forest Classifier is', round(accuracy_score(prediction_rm,y_test)*100,2))\nkfold = KFold(n_splits=10,random_state=22)  # k=10, split the data into 10 equal parts\nresult_rm = cross_val_score(model,all_features,Targeted_feature,cv=10)\nprint('The cross validated score for Random Forest Classifier is:',round(result_rm.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap='summer')\nplt.title('Confusion_matrix',y=1.05,size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Support Vector Machines**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC, LinearSVC\n\nmodel = SVC()\nmodel.fit(X_train,y_train)\nprediction_svm = model.predict(X_test)\nprint('-----------------Accuracy of the model-----------------')\nprint('The accuracy of Support Vector Machines Classifier is', round(accuracy_score(prediction_svm,y_test)*100,2))\nkfold = KFold(n_splits=10,random_state=22)  # k=10, split the data into 10 equal parts\nresult_svm = cross_val_score(model,all_features,Targeted_feature,cv=10)\nprint('The cross validated score for Support Vector Machines Classifier is:',round(result_svm.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap='summer')\nplt.title('Confusion_matrix',y=1.05,size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**KNN Classifer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors = 4)\nmodel.fit(X_train,y_train)\nprediction_knn = model.predict(X_test)\nprint('-----------------Accuracy of the model-----------------')\nprint('The accuracy of KNN Classifier is', round(accuracy_score(prediction_knn,y_test)*100,2))\nkfold = KFold(n_splits=10,random_state=22)  # k=10, split the data into 10 equal parts\nresult_knn = cross_val_score(model,all_features,Targeted_feature,cv=10)\nprint('The cross validated score for KNN Classifier is:',round(result_knn.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap='summer')\nplt.title('Confusion_matrix',y=1.05,size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gaussian Naive Bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\nmodel.fit(X_train,y_train)\nprediction_GNB = model.predict(X_test)\nprint('-----------------Accuracy of the model-----------------')\nprint('The accuracy of Gaussian Naive Bayes Classifier is', round(accuracy_score(prediction_GNB,y_test)*100,2))\nkfold = KFold(n_splits=10,random_state=22)  # k=10, split the data into 10 equal parts\nresult_GNB = cross_val_score(model,all_features,Targeted_feature,cv=10)\nprint('The cross validated score for Gaussian Naive Bayes Classifier is:',round(result_GNB.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap='summer')\nplt.title('Confusion_matrix',y=1.05,size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmodel = RandomForestClassifier(criterion='gini', min_samples_split=10,min_samples_leaf=1,max_features='auto')\nmodel.fit(X_train,y_train)\nprediction_tree = model.predict(X_test)\nprint('-----------------Accuracy of the model-----------------')\nprint('The accuracy of Decision Tree Classifier is', round(accuracy_score(prediction_tree,y_test)*100,2))\nkfold = KFold(n_splits=10,random_state=22)  # k=10, split the data into 10 equal parts\nresult_tree = cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Decision Tree Classifier is:',round(result_tree.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap='summer')\nplt.title('Confusion_matrix',y=1.05,size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ada Boost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nmodel = AdaBoostClassifier()\nmodel.fit(X_train,y_train)\nprediction_adb = model.predict(X_test)\nprint('-----------------Accuracy of the model-----------------')\nprint('The accuracy of Ada Boost Classifier is', round(accuracy_score(prediction_adb,y_test)*100,2))\nkfold = KFold(n_splits=10,random_state=22)  # k=10, split the data into 10 equal parts\nresult_adb = cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Decision Tree Classifier is:',round(result_adb.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap='summer')\nplt.title('Confusion_matrix',y=1.05,size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear Discriminant Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nmodel = LinearDiscriminantAnalysis()\nmodel.fit(X_train,y_train)\nprediction_lda = model.predict(X_test)\nprint('-----------------Accuracy of the model-----------------')\nprint('The accuracy of Linear Discriminant Analysis Classifier is', round(accuracy_score(prediction_lda,y_test)*100,2))\nkfold = KFold(n_splits=10,random_state=22)  # k=10, split the data into 10 equal parts\nresult_lda = cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Decision Tree Classifier is:',round(result_lda.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap='summer')\nplt.title('Confusion_matrix',y=1.05,size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradient Boosting/Descent Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nmodel = GradientBoostingClassifier()\nmodel.fit(X_train,y_train)\nprediction_gbc = model.predict(X_test)\nprint('-----------------Accuracy of the model-----------------')\nprint('The accuracy of Gradient Boosting Classifier is', round(accuracy_score(prediction_gbc,y_test)*100,2))\nkfold = KFold(n_splits=10,random_state=22)  # k=10, split the data into 10 equal parts\nresult_gbc = cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Decision Tree Classifier is:',round(result_gbc.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap='summer')\nplt.title('Confusion_matrix',y=1.05,size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Evaluation**\nWe can now rank our evaluation of all the models to choose the best for our problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Random Forest', 'Naive Bayes', 'AdaBoostClassifier', 'Gradient Decent',\n             'Linear Discriminant Analysis', 'Decision Tree'],\n    'Score': [result_svm.mean(), result_knn.mean(), result_lr.mean(), result_rm.mean(), result_GNB.mean(), result_adb.mean(), result_gbc.mean(),\n             result_lda.mean(), result_tree.mean()]})\nmodels.sort_values(by='Score',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at all the matrices, Random Forest & SVM classifer has a higher chance in correctly predicting dead passengers."},{"metadata":{},"cell_type":"markdown","source":"**Hyper-Parameters Tuning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = traindf.drop('Survived', axis=1)\ntrain_Y = traindf['Survived']\ntest_X = testdf.drop('PassengerId', axis=1).copy()\ntrain_X.shape, train_Y.shape, test_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient boosting tuning\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel = GradientBoostingClassifier()\nparam_grid = {'loss' : ['deviance'],\n              'n_estimators' : [100,200,300,400],\n              'learning_rate' : [0.1,0.05,0.01,0.001],\n              'max_depth' : [4,8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3,0.2,0.1]\n}\n\nmodelf = GridSearchCV(model,param_grid = param_grid, cv=kfold, scoring='accuracy', n_jobs=4, verbose=1)\n\nmodelf.fit(train_X,train_Y)\nmodelf.best_score_\nmodelf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelf.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Classifier Parameters Tuning\nmodel = RandomForestClassifier()\nn_estim = range(100,1000,100)\n\n# Search grid for optimal parameters\nparam_grid = {'n_estimators' :n_estim}\n\nmodel_rf = GridSearchCV(model, param_grid = param_grid, cv=5, scoring='accuracy', n_jobs=4, verbose=1)\nmodel_rf.fit(train_X,train_Y)\n\nprint(model_rf.best_score_)\nmodel_rf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearDiscriminantAnalysis()\nparam_grid = {'tol':[0.001,0.01,.1,.2]}\n\nmodel = GridSearchCV(model,param_grid, cv=5, scoring='accuracy',n_jobs=4,verbose=1)\nmodel.fit(train_X, train_Y)\nprint(model.best_score_)\nmodel.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SVC()\nparam_grid = {'kernel':['rbf','linear'],\n              'gamma':[0.001,0.01,0.1,1],\n              'C':[1,10,50,100,200,300,1000]}\n\nmodel_svm = GridSearchCV(model,param_grid = param_grid, cv=5, scoring='accuracy', n_jobs=4, verbose=1)\nmodel_svm.fit(train_X,train_Y)\nprint(model_svm.best_estimator_)\nprint(modelsvm.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apply the Estimator which got from parameter tuning of Random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forests\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=800, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nrandom_forest.fit(train_X,train_Y)\nY_pred_rf = random_forest.predict(test_X)\nrandom_forest.score(train_X,train_Y)\nacc_random_forest = round(random_forest.score(train_X,train_Y)*100,2)\n\nprint('Important features')\npd.Series(random_forest.feature_importances_,train_X.columns).sort_values(ascending=True).plot.barh(width=0.8)\nprint('--'*30)\nprint(acc_random_forest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n                'PassengerID': test_df['PassengerId'],\n                'Survived' : Y_pred_rf\n})\nsubmission.to_csv('Titanic_90%_accuracy.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}